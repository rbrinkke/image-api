# Image Processor Service

**A production-ready, domain-agnostic microservice for intelligent image processing**

## Overview

Image Processor is a standalone FastAPI-based microservice designed to handle image uploads, processing, and retrieval with enterprise-grade reliability. Built with a metadata-driven architecture, it processes images through EXIF stripping, multi-size generation, and modern format conversion while maintaining complete separation from your application's business logic.

### Key Characteristics

- **Domain-Agnostic**: No knowledge of users, activities, or business entities - pure utility service
- **Standalone**: Self-contained with embedded SQLite database and local/S3 storage
- **Async-First**: Non-blocking API with Celery workers for CPU-intensive processing
- **Storage-Agnostic**: Seamless switching between local filesystem and AWS S3
- **Production-Ready**: Built-in rate limiting, retry logic, health monitoring, and audit trails

## Architecture

```
┌──────────────────────────────────────────────────────────┐
│                    FastAPI Gateway                        │
│  • JWT Authentication          • Rate Limiting            │
│  • Content Validation          • Multi-layer Security     │
└────────────────────┬─────────────────────────────────────┘
                     │
                     ├──> SQLite Database (Embedded)
                     │    • processing_jobs (state machine)
                     │    • image_upload_events (audit trail)
                     │    • upload_rate_limits (enforcement)
                     │
                     ├──> Storage Layer (Abstraction)
                     │    • Local Filesystem (development)
                     │    • AWS S3 (production)
                     │    • Zero code changes to migrate
                     │
                     └──> Redis (Task Queue Only)
                          • Celery broker for async jobs
                          • Transient message passing

┌──────────────────────────────────────────────────────────┐
│                   Celery Workers                          │
│  • EXIF Metadata Stripping    • Multi-Size Generation    │
│  • WebP Conversion             • Dominant Color Extract   │
│  • Pillow-SIMD Optimized      • Automatic Retry Logic    │
└──────────────────────────────────────────────────────────┘
```

## Core Features

### Security-First Design

- **JWT-Based Authentication**: Token validation with configurable secrets
- **Magic Bytes Validation**: Never trust client-provided MIME types
- **EXIF Metadata Stripping**: Removes GPS coordinates, camera info, and potential malicious payloads
- **Content-Length Pre-Check**: Reject oversized uploads before bandwidth consumption
- **Database-Enforced Rate Limiting**: 50 uploads/hour per user (configurable)
- **Stream Rewind Pattern**: Ensures validation doesn't corrupt processing pipeline

### Intelligent Image Processing

**Multi-Size Generation:**
- `thumbnail`: 150px - List/grid views
- `medium`: 600px - Detail views
- `large`: 1200px - Full-screen views
- `original`: 4096px - Backup/archival

**Format Optimization:**
- Automatic WebP conversion (25-35% smaller than JPEG)
- Configurable quality settings (default: 85)
- Pillow-SIMD support for 4-6x performance boost
- Aspect ratio preservation across all sizes

**Smart Metadata Extraction:**
- Dominant color calculation for progressive loading placeholders
- Dimension tracking per variant
- File size metrics for bandwidth optimization

### Async Processing Pipeline

```
Client Upload
    ↓
API: Validate + Stage (< 50ms)
    ↓
Return 202 Accepted (job_id, image_id)
    ↓
Celery Worker: Process in Background (3-5s)
    ├─ Load from staging
    ├─ Strip EXIF metadata
    ├─ Extract dominant color
    ├─ Generate 4 size variants
    ├─ Convert to WebP
    ├─ Upload to final storage
    └─ Update database status
    ↓
Client Polls: GET /jobs/{job_id}/result
```

### Storage Abstraction

**Protocol-Based Design** allows zero-code migration:

```python
# Development (Local)
STORAGE_BACKEND=local
STORAGE_PATH=/data/storage

# Production (S3)
STORAGE_BACKEND=s3
AWS_REGION=eu-west-1

# Same codebase, no changes needed
```

**Storage Features:**
- UUID-based filenames (unpredictable, collision-free)
- Hierarchical organization by variant type
- Server-side encryption (SSE-S3) when using AWS
- Presigned URLs for S3 direct access

## API Specification

### Upload Endpoint

```bash
POST /api/v1/images/upload
Content-Type: multipart/form-data
Authorization: Bearer {jwt_token}

Parameters:
  - file: binary (required)
  - bucket: string (required) - Target storage bucket
  - metadata: json (optional) - Pass-through context

Response (202 Accepted):
{
  "job_id": "uuid",
  "image_id": "uuid",
  "status_url": "/api/v1/images/jobs/{job_id}",
  "message": "Upload accepted. Processing initiated."
}

Headers:
  X-RateLimit-Limit: 50
  X-RateLimit-Remaining: 49
  X-RateLimit-Reset: 2024-11-08T15:00:00Z
```

### Status Polling

```bash
GET /api/v1/images/jobs/{job_id}

Response:
{
  "job_id": "uuid",
  "image_id": "uuid",
  "status": "pending|processing|completed|failed",
  "created_at": "2024-11-08T14:00:00Z",
  "completed_at": "2024-11-08T14:00:05Z",
  "error": null,
  "attempts": 0
}
```

### Retrieve Results

```bash
GET /api/v1/images/jobs/{job_id}/result

Response (200 OK):
{
  "image_id": "uuid",
  "status": "completed",
  "urls": {
    "thumbnail": "/storage/bucket/processed/thumbnail/uuid_thumbnail.webp",
    "medium": "/storage/bucket/processed/medium/uuid_medium.webp",
    "large": "/storage/bucket/processed/large/uuid_large.webp",
    "original": "/storage/bucket/processed/original/uuid_original.webp"
  },
  "metadata": {
    "dominant_color": "#3A5F8C",
    "original_dimensions": {
      "width": 3024,
      "height": 4032
    },
    "variants": {
      "thumbnail": {
        "width": 150,
        "height": 200,
        "aspect_ratio": 0.75,
        "format": "webp",
        "size_bytes": 12453
      },
      "medium": { ... },
      "large": { ... },
      "original": { ... }
    }
  },
  "completed_at": "2024-11-08T14:00:05Z"
}
```

### Retrieval by Image ID

```bash
# Get specific size variant
GET /api/v1/images/{image_id}?size=medium

# Get all sizes
GET /api/v1/images/{image_id}/all

# Direct file serving
GET /api/v1/images/{image_id}/direct?size=large

# Batch retrieval (max 50)
GET /api/v1/images/batch?image_ids=uuid1,uuid2,uuid3&size=medium

# Delete image and all variants
DELETE /api/v1/images/{image_id}
```

### Health & Monitoring

```bash
# Basic health check
GET /api/v1/health

# Detailed statistics
GET /api/v1/health/stats

# Failed jobs debugging
GET /api/v1/health/failed?limit=50
```

## Database Schema

### SQLite Tables

**processing_jobs** - Core state machine
```sql
- job_id: TEXT PRIMARY KEY
- image_id: TEXT (logical identifier)
- status: pending|processing|completed|failed|retrying
- storage_bucket: TEXT
- staging_path: TEXT (temporary location)
- processed_paths: TEXT (JSON: {variant: path})
- processing_metadata: TEXT (JSON: pass-through + generated)
- attempt_count: INTEGER
- max_retries: INTEGER (default: 3)
- last_error: TEXT
- created_at, started_at, completed_at: TEXT (ISO timestamps)
```

**image_upload_events** - Audit trail
```sql
- id: TEXT PRIMARY KEY
- event_type: upload_initiated|processing_started|completed|failed
- image_id: TEXT
- job_id: TEXT
- metadata: TEXT (JSON)
- created_at: TEXT
```

**upload_rate_limits** - Enforcement
```sql
- user_id: TEXT
- window_start: TEXT (hourly window)
- upload_count: INTEGER
- PRIMARY KEY (user_id, window_start)
```

## Quick Start

### Prerequisites

- Docker & Docker Compose
- 2GB RAM minimum (4GB recommended)
- 10GB disk space for development

### Installation

```bash
# 1. Clone repository
git clone https://github.com/yourusername/image-processor.git
cd image-processor

# 2. Create environment file
cp .env.example .env

# Edit .env and set:
# JWT_SECRET_KEY=your-secure-random-key-here

# 3. Start services
docker-compose up -d

# 4. Verify health
curl http://localhost:8000/api/v1/health

# 5. View API documentation
open http://localhost:8000/docs

# 6. Monitor Celery workers
open http://localhost:5555  # Flower dashboard
```

### Test Upload

```bash
# Generate test JWT
export TEST_TOKEN=$(python3 -c "
import jwt
print(jwt.encode(
    {'sub': 'test-user-123'}, 
    'dev-secret-change-in-production', 
    algorithm='HS256'
))
")

# Upload test image
curl -X POST http://localhost:8000/api/v1/images/upload \
  -H "Authorization: Bearer $TEST_TOKEN" \
  -F "file=@test-photo.jpg" \
  -F "bucket=test-uploads" \
  -F "metadata={\"context\":\"test_upload\"}"

# Response:
# {
#   "job_id": "abc-123...",
#   "image_id": "def-456...",
#   "status_url": "/api/v1/images/jobs/abc-123..."
# }

# Check status (repeat until completed)
curl http://localhost:8000/api/v1/images/jobs/abc-123...

# Get processed results
curl http://localhost:8000/api/v1/images/jobs/abc-123.../result
```

## Configuration

### Environment Variables

```bash
# Service Identity
SERVICE_NAME=image-processor
VERSION=1.0.0

# Database
DATABASE_PATH=/data/processor.db

# Storage Backend
STORAGE_BACKEND=local              # or 's3'
STORAGE_PATH=/data/storage          # for local backend
AWS_REGION=eu-west-1                # for S3 backend

# Redis
REDIS_URL=redis://redis:6379/0

# Security
JWT_SECRET_KEY=CHANGE_THIS_IN_PRODUCTION
JWT_ALGORITHM=HS256

# Rate Limiting
RATE_LIMIT_MAX_UPLOADS=50
RATE_LIMIT_WINDOW_MINUTES=60

# Upload Constraints
MAX_UPLOAD_SIZE_MB=10
ALLOWED_MIME_TYPES=["image/jpeg","image/png","image/webp"]

# Processing Configuration
WEBP_QUALITY=85
IMAGE_SIZES={"thumbnail":150,"medium":600,"large":1200,"original":4096}
```

## Deployment

### Development

```bash
# Hot-reload enabled
docker-compose up

# View logs
docker-compose logs -f api
docker-compose logs -f worker

# Scale workers
docker-compose up --scale worker=4
```

### Production

**1. Security Hardening**

```bash
# Generate secure JWT secret
openssl rand -hex 32

# Update .env
JWT_SECRET_KEY=<generated-secret>
```

**2. Storage Migration**

```bash
# Switch to S3
STORAGE_BACKEND=s3
AWS_REGION=eu-west-1

# Configure AWS credentials
AWS_ACCESS_KEY_ID=<your-key>
AWS_SECRET_ACCESS_KEY=<your-secret>
```

**3. CORS Configuration**

```python
# app/main.py
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "https://yourdomain.com",
        "https://app.yourdomain.com"
    ],
    allow_credentials=True,
    allow_methods=["POST", "GET", "DELETE"],
    allow_headers=["Authorization", "Content-Type"],
)
```

**4. Reverse Proxy (Nginx)**

```nginx
upstream image_processor {
    server localhost:8000;
}

server {
    listen 443 ssl http2;
    server_name processor.yourdomain.com;

    ssl_certificate /path/to/cert.pem;
    ssl_certificate_key /path/to/key.pem;

    client_max_body_size 10M;

    location / {
        proxy_pass http://image_processor;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    location /storage/ {
        alias /data/storage/;
        expires 1y;
        add_header Cache-Control "public, immutable";
    }
}
```

**5. Monitoring & Logging**

```yaml
# docker-compose.prod.yml
services:
  api:
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    restart: unless-stopped

  worker:
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"
    restart: unless-stopped
```

**6. Backup Strategy**

```bash
# Backup SQLite database
sqlite3 /data/processor.db ".backup /backup/processor-$(date +%Y%m%d).db"

# Backup storage (rsync for local, AWS S3 replication for cloud)
rsync -av /data/storage/ /backup/storage/

# Automated daily backups
0 2 * * * /scripts/backup.sh
```

## Performance Optimization

### Pillow-SIMD Installation

For 4-6x faster image processing:

```dockerfile
# Dockerfile
FROM python:3.11-slim

# ... system dependencies ...

# Uninstall stock Pillow, install SIMD version
RUN pip uninstall -y pillow && \
    pip install pillow-simd --no-cache-dir

# Requires AVX2 CPU support (most modern processors)
```

### Worker Scaling

```bash
# Horizontal scaling (multiple containers)
docker-compose up --scale worker=8

# Vertical scaling (concurrency per container)
# docker-compose.yml
services:
  worker:
    command: celery -A app.tasks.celery_app worker --concurrency=4
    # Set concurrency = number of CPU cores
```

### Database Optimization

```sql
-- Enable WAL mode for better concurrency
PRAGMA journal_mode=WAL;

-- Analyze query plans
EXPLAIN QUERY PLAN 
SELECT * FROM processing_jobs WHERE status = 'pending';
```

## Integration Guide

### Client Application (Your Activity App)

**Upload Flow:**

```python
import httpx

class ImageProcessorClient:
    def __init__(self, base_url: str, jwt_token: str):
        self.base_url = base_url
        self.headers = {"Authorization": f"Bearer {jwt_token}"}
    
    async def upload_image(
        self, 
        file_data: bytes, 
        bucket: str, 
        metadata: dict = None
    ) -> dict:
        """Upload image and return job info"""
        files = {"file": ("upload.jpg", file_data, "image/jpeg")}
        data = {
            "bucket": bucket,
            "metadata": json.dumps(metadata or {})
        }
        
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.base_url}/api/v1/images/upload",
                files=files,
                data=data,
                headers=self.headers
            )
            response.raise_for_status()
            return response.json()
    
    async def poll_until_complete(
        self, 
        job_id: str, 
        max_attempts: int = 30
    ) -> dict:
        """Poll job status until completed"""
        for _ in range(max_attempts):
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    f"{self.base_url}/api/v1/images/jobs/{job_id}/result",
                    headers=self.headers
                )
                
                if response.status_code == 200:
                    return response.json()
                elif response.status_code == 409:
                    await asyncio.sleep(2)
                    continue
                else:
                    response.raise_for_status()
        
        raise TimeoutError(f"Job {job_id} did not complete in time")

# Usage in your application
processor = ImageProcessorClient(
    base_url="http://processor:8000",
    jwt_token=your_jwt
)

# Upload
upload_result = await processor.upload_image(
    file_data=image_bytes,
    bucket="activity-images",
    metadata={"activity_id": "xyz", "user_id": "abc"}
)

# Wait for processing
result = await processor.poll_until_complete(upload_result["job_id"])

# Store in your MongoDB/SQLite
await db.activities.update_one(
    {"_id": activity_id},
    {
        "$set": {
            "cover_image_id": result["image_id"],
            "cover_image_urls": result["urls"],
            "cover_image_color": result["metadata"]["dominant_color"]
        }
    }
)
```

### Webhook Support (Future Enhancement)

```python
# Instead of polling, receive webhook when processing completes
@router.post("/webhooks/image-processor")
async def image_processor_webhook(payload: dict):
    """Processor notifies when job completes"""
    if payload["status"] == "completed":
        # Update your database
        await update_activity_images(
            image_id=payload["image_id"],
            urls=payload["urls"]
        )
```

## Troubleshooting

### Common Issues

**1. "File too large" errors**
```bash
# Increase in .env
MAX_UPLOAD_SIZE_MB=20

# Also update Nginx
client_max_body_size 20M;
```

**2. Workers not processing**
```bash
# Check Redis connection
docker-compose exec worker redis-cli -h redis ping

# Check Celery workers
docker-compose exec worker celery -A app.tasks.celery_app inspect active

# View Flower dashboard
open http://localhost:5555
```

**3. Rate limit errors**
```bash
# Increase limits
RATE_LIMIT_MAX_UPLOADS=100

# Or reset for specific user
sqlite3 /data/processor.db "DELETE FROM upload_rate_limits WHERE user_id='user-id';"
```

**4. Slow processing**
```bash
# Install Pillow-SIMD
pip uninstall pillow
pip install pillow-simd

# Increase worker concurrency
docker-compose up --scale worker=4
```

**5. Database locked errors**
```bash
# Enable WAL mode
sqlite3 /data/processor.db "PRAGMA journal_mode=WAL;"

# Check for long-running queries
sqlite3 /data/processor.db "SELECT * FROM processing_jobs WHERE status='processing' AND started_at < datetime('now', '-10 minutes');"
```

## Monitoring & Observability

### Health Endpoints

```bash
# Basic health
curl http://localhost:8000/api/v1/health

# Detailed statistics
curl http://localhost:8000/api/v1/health/stats | jq

{
  "status_breakdown": {
    "completed": 1543,
    "pending": 2,
    "processing": 1,
    "failed": 12
  },
  "performance_24h": {
    "completed_24h": 234,
    "avg_processing_time_seconds": 4.23
  },
  "storage": {
    "total_images": 1543,
    "total_jobs": 1558
  },
  "celery": {
    "active_workers": 4,
    "active_tasks": 3
  }
}
```

### Flower Monitoring

Access Celery Flower dashboard at `http://localhost:5555`

- Real-time worker status
- Task history and timing
- Failed task inspection
- Worker resource utilization

### Log Aggregation

```bash
# View all logs
docker-compose logs -f

# Specific service logs
docker-compose logs -f worker

# Search for errors
docker-compose logs worker | grep ERROR

# Export logs
docker-compose logs --no-color > processor-logs.txt
```

## Testing

### Unit Tests

```bash
# Run test suite
pytest tests/ -v

# With coverage
pytest tests/ --cov=app --cov-report=html

# Specific test
pytest tests/test_upload.py::test_rate_limiting -v
```

### Integration Tests

```bash
# End-to-end upload test
python tests/integration/test_e2e_upload.py

# Load testing
locust -f tests/load/locustfile.py --host=http://localhost:8000
```

## Performance Benchmarks

**Hardware:** 4-core CPU, 8GB RAM, SSD storage

| Operation | Time | Throughput |
|-----------|------|------------|
| Upload API (validation only) | ~50ms | 1000 req/s |
| Complete processing (2MB JPEG) | ~4.5s | - |
| Thumbnail generation | ~200ms | - |
| Medium variant | ~600ms | - |
| Large variant | ~1.2s | - |
| EXIF stripping | ~100ms | - |
| Database query (status check) | ~2ms | 5000 req/s |

**With Pillow-SIMD:**
- 4-6x faster image operations
- Processing time: ~1.8s (vs 4.5s)

## Roadmap

### Planned Features

- [ ] Webhook callbacks for completion notifications
- [ ] Batch upload endpoint (multiple images in single request)
- [ ] Image transformation options (crop, rotate, filters)
- [ ] Video thumbnail generation
- [ ] AVIF format support (next-gen compression)
- [ ] CDN integration (CloudFront, Cloudflare)
- [ ] Metrics export (Prometheus)
- [ ] Admin UI for job management

### Migration Path

**From Local to S3:**
```bash
# 1. Copy existing files to S3
aws s3 sync /data/storage/ s3://your-bucket/

# 2. Update configuration
STORAGE_BACKEND=s3

# 3. Restart services
docker-compose restart
```

## Security Considerations

### Best Practices

1. **JWT Secret Rotation**: Change `JWT_SECRET_KEY` periodically
2. **HTTPS Only**: Never expose over HTTP in production
3. **Rate Limiting**: Monitor and adjust based on abuse patterns
4. **Input Validation**: Magic bytes check prevents malicious files
5. **EXIF Stripping**: Removes GPS and sensitive metadata
6. **Audit Trail**: All events logged in `image_upload_events`

### Known Limitations

- SQLite write-locking (acceptable for <100 req/s)
- Rate limits stored in-memory (Redis) - lost on restart
- No built-in image virus scanning (add ClamAV if needed)

## License

MIT License - see [LICENSE](LICENSE) file

## Support

- **Documentation**: [https://docs.yourproject.com](https://docs.yourproject.com)
- **Issues**: [https://github.com/yourusername/image-processor/issues](https://github.com/yourusername/image-processor/issues)
- **Discussions**: [https://github.com/yourusername/image-processor/discussions](https://github.com/yourusername/image-processor/discussions)

## Contributing

Contributions welcome! Please read [CONTRIBUTING.md](CONTRIBUTING.md) first.

## Acknowledgments

Built with:
- [FastAPI](https://fastapi.tiangolo.com/) - Modern Python web framework
- [Celery](https://docs.celeryproject.org/) - Distributed task queue
- [Pillow](https://pillow.readthedocs.io/) - Python imaging library
- [SQLite](https://www.sqlite.org/) - Embedded database engine
- [Redis](https://redis.io/) - In-memory data structure store

---

**Built with ❤️ for developers who need reliable image processing without the complexity.**
